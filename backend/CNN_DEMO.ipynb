{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee766442",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90d389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9509f",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac445264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API and processing parameters\n",
    "API_KEY = \"API_KEY\"\n",
    "ZOOM_LEVEL = 18\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 128 if len(tf.config.list_physical_devices('GPU')) > 0 else 64 # Adjust based on GPU availability\n",
    "EPOCHS = 20\n",
    "PATIENCE = 5\n",
    "CLASS_NAMES = ['class0', 'class1', 'class2', 'class3']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "output_dir = \"GuadalaHacks 2025\"\n",
    "\n",
    "# Loading pre-adjusted POI data and gdf with POI points and labels\n",
    "def load_poi_data(geojson_path):\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    return gdf[gdf.geometry.type == 'Point']\n",
    "\n",
    "# Convert geographic coordinates to tile indices\n",
    "def latlon_to_tile(lat, lon, zoom):\n",
    "    lat_rad = math.radians(lat)\n",
    "    n = 2 ** zoom\n",
    "    x = int((lon + 180.0) / 360.0 * n)\n",
    "    y = int((1.0 - math.asinh(math.tan(lat_rad)) / (2 * math.pi)) * n)\n",
    "    return x, y\n",
    "\n",
    "# Download satellite tile for given coordinates\n",
    "def download_tile_image(lon, lat):\n",
    "    try:\n",
    "        x, y = latlon_to_tile(lat, lon, ZOOM_LEVEL)\n",
    "        url = f\"https://maps.hereapi.com/v3/base/mc/{ZOOM_LEVEL}/{x}/{y}/png?apiKey={API_KEY}&style=satellite.day&size=512\"\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        return Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image for ({lon:.4f}, {lat:.4f}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "### Dataset generation ###\n",
    "# Process and save POI data\n",
    "def process_poi(poi, output_dir, idx):\n",
    "    try:\n",
    "        # Extract coordinates from geometry\n",
    "        lon = poi.geometry.x\n",
    "        lat = poi.geometry.y\n",
    "        \n",
    "        # Download and process image\n",
    "        img = download_tile_image(lon, lat)\n",
    "        if img:\n",
    "            # Resize and normalize\n",
    "            img = img.resize(IMAGE_SIZE)\n",
    "            img_array = np.array(img) / 255.0\n",
    "            \n",
    "            # Get label and create directory\n",
    "            label = int(poi['LABEL'])\n",
    "            class_dir = os.path.join(output_dir, CLASS_NAMES[label])\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            \n",
    "            # Save compressed data\n",
    "            np.savez_compressed(\n",
    "                os.path.join(class_dir, f'poi_{idx:06d}.npz'),\n",
    "                image=img_array,\n",
    "                label=label\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing POI {idx}: {str(e)}\")\n",
    "\n",
    "# Create datset using parallel processing\n",
    "def generate_dataset(pois, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process in chunks to handle large datasets\n",
    "    chunk_size = 1000\n",
    "    total_pois = len(pois)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        for chunk_start in range(0, total_pois, chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, total_pois)\n",
    "            futures = []\n",
    "            \n",
    "            # Submit chunk tasks\n",
    "            for idx in range(chunk_start, chunk_end):\n",
    "                futures.append(executor.submit(\n",
    "                    process_poi,\n",
    "                    pois.iloc[idx],\n",
    "                    output_dir,\n",
    "                    idx\n",
    "                ))\n",
    "            \n",
    "            # Monitor progress\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Processing error: {str(e)}\")\n",
    "\n",
    "### Model definition ###\n",
    "# Build classification model using EfficientNetB0\n",
    "def build_classification_model():\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(*IMAGE_SIZE, 3)\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning configuration\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu', \n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "\n",
    "### Training Pipeline ###\n",
    "# Calculate class weights for imbalanced datasets\n",
    "def calculate_class_weights(data_dir):\n",
    "    class_counts = [len(os.listdir(os.path.join(data_dir, cls))) \n",
    "                   for cls in CLASS_NAMES]\n",
    "    total_samples = sum(class_counts)\n",
    "    return {i: total_samples/(NUM_CLASSES * count) \n",
    "           for i, count in enumerate(class_counts) if count > 0}\n",
    "\n",
    "# Optimized training workflow\n",
    "def train_model(data_dir):\n",
    "    # Enable hardware accelerations\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "    # Create data pipelines\n",
    "    def preprocess(image, label):\n",
    "        return image, tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='int',\n",
    "        validation_split=0.2,\n",
    "        subset='training',\n",
    "        shuffle=True,\n",
    "        seed=42).map(preprocess).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='int',\n",
    "        validation_split=0.2,\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    ).map(preprocess).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Model configuration\n",
    "    model = build_classification_model()\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Nadam(learning_rate=1e-3),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy(name='accuracy')]\n",
    "    )\n",
    "\n",
    "    # Training callbacks\n",
    "    callbacks_list = [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=PATIENCE,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            'best_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True\n",
    "        ),\n",
    "        callbacks.TensorBoard(\n",
    "            log_dir='training_logs',\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = calculate_class_weights(data_dir)\n",
    "\n",
    "    # Start training\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks_list,\n",
    "        class_weight=class_weights,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "### Prediction and evaluation ###\n",
    "# Load trained model with custom objects\n",
    "def load_trained_model(model_path='poi_classifier_final.h5'):\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Make predictions on new unlabeled POIs\n",
    "def predict_new_pois(model, geojson_path, output_csv='predictions.csv'):\n",
    "    # Load new POIs\n",
    "    new_pois = load_poi_data(geojson_path)\n",
    "    \n",
    "    # Temporary directory for processing\n",
    "    temp_dir = 'temp_predictions'\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Create dummy structure for dataset loader\n",
    "    dummy_class_dir = os.path.join(temp_dir, 'dummy_class')\n",
    "    os.makedirs(dummy_class_dir, exist_ok=True)\n",
    "    \n",
    "    # Process and save images\n",
    "    for idx, poi in new_pois.iterrows():\n",
    "        lon = poi.geometry.x\n",
    "        lat = poi.geometry.y\n",
    "        img = download_tile_image(lon, lat)\n",
    "        if img:\n",
    "            img = img.resize(IMAGE_SIZE)\n",
    "            img_array = np.array(img) / 255.0\n",
    "            np.savez_compressed(\n",
    "                os.path.join(dummy_class_dir, f'pred_{idx}.npz'),\n",
    "                image=img_array\n",
    "            )\n",
    "    \n",
    "    # Create dataset\n",
    "    pred_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        temp_dir,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    probabilities = model.predict(pred_ds)\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    # Cleanup\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "    # Add predictions to GeoDataFrame\n",
    "    new_pois['prediction'] = predictions\n",
    "    new_pois['confidence'] = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Save results\n",
    "    new_pois.to_csv(output_csv)\n",
    "    return new_pois\n",
    "\n",
    "# Evaluate model performance on test data\n",
    "def evaluate_model(model, test_geojson_path):\n",
    "    # Load and process test data\n",
    "    test_pois = load_poi_data(test_geojson_path)\n",
    "    test_dir = 'temp_evaluation'\n",
    "    \n",
    "    # Create labeled dataset\n",
    "    generate_dataset(test_pois, test_dir)\n",
    "    \n",
    "    # Load evaluation dataset\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        label_mode='int',\n",
    "        shuffle=False\n",
    "    ).map(lambda x, y: (x, tf.one_hot(y, NUM_CLASSES)))\n",
    "    \n",
    "    # Get true labels and predictions\n",
    "    y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    y_pred = model.predict(test_ds)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Generate metrics\n",
    "    report = classification_report(\n",
    "        y_true_labels,\n",
    "        y_pred_labels,\n",
    "        target_names=CLASS_NAMES,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=CLASS_NAMES,\n",
    "                yticklabels=CLASS_NAMES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Cleanup\n",
    "    shutil.rmtree(test_dir)\n",
    "    \n",
    "    return report, cm\n",
    "\n",
    "### Implementation ###\n",
    "# Load trained model\n",
    "model = load_trained_model()\n",
    "    \n",
    "# OPTION 1: Predict new POIs\n",
    "predictions = predict_new_pois(\n",
    "    model,\n",
    "    'new_pois.geojson',\n",
    "    output_csv='new_predictions.csv'\n",
    "    )\n",
    "    \n",
    "# OPTION 2: Evaluate on test data\n",
    "test_report, test_cm = evaluate_model(\n",
    "    model,\n",
    "    'test_pois.geojson'\n",
    "    )\n",
    "    \n",
    "# Print evaluation results\n",
    "print(\"\\nClassification Report:\")\n",
    "print(pd.DataFrame(test_report).transpose())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(test_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
